{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5fcab1c8-5e21-42ad-9c7e-e64b376bdd0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --upgrade tensorflow-gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cf5e47f7-b751-4972-81bf-e8f6021adf0c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import re\n",
    "import imageio\n",
    "import unicodedata\n",
    "import ast\n",
    "import itertools\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from PIL import UnidentifiedImageError\n",
    "from xml.dom import minidom\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('dark_background')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d53d29a-c2ea-40a9-96ea-c96fe0816a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load from dynamic kernels code\n",
    "sys.path.insert(0, '/project/dynamic-kernels')\n",
    "\n",
    "from src.layers import CADenseAdd, CADenseMul\n",
    "from src.optimizers import SVDAdam, SVDSGD\n",
    "# from src.models import CAEncoderLayer, PositionalEncodingLayer, EncoderLayer\n",
    "from src.models.utils import wrap_model\n",
    "# from src.callbacks import ReduceLROPlateau"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0643cf7-babc-49f8-a650-71147b77456d",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b265a06d-4821-44f0-ac51-93f270f87ddb",
   "metadata": {},
   "source": [
    "Context tags data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "df6cf381-9e29-4db6-bdbd-b43617cfa510",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10005385    [arizona, desert, cholla, cactus, teddybear, b...\n",
       "10005386    [arizona, desert, sonoran, clouds, mountain, s...\n",
       "Name: tags, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Amount of tag chunks\n",
    "n_chunks = 83\n",
    "# Get tags from chunks\n",
    "tags = pd.concat([\n",
    "    pd.read_csv(f'tags/chunk_{chunk}', index_col=0).rename({'link': 'tags'}, axis=1) \n",
    "    for chunk in range(n_chunks)\n",
    "])\n",
    "# set index of tags to index of urls\n",
    "tags.index = tags.index.astype(str)\n",
    "# Groupby index and join tags\n",
    "tags = tags.groupby(level=0).tags.apply(list)\n",
    "# Evaluate str to list\n",
    "tags = tags.map(lambda l: ast.literal_eval(l[0]))\n",
    "# Show examples\n",
    "tags.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc65f255-d82d-4193-aa33-394d8df6931f",
   "metadata": {},
   "source": [
    "Original Flickr urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e5076458-701c-4b6f-b752-979ae9c2102c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43398    [https://www.flickr.com/photos/george/43398/]\n",
       "Name: link, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get urls for indices\n",
    "urls = pd.read_csv('data/gps_urls.txt', header=None, delimiter=' ', names=['link'])\n",
    "# # Groupby index and join links\n",
    "urls = urls.groupby(level=0).link.apply(list)\n",
    "# Show examples\n",
    "urls.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e24b2a-f70c-44aa-b692-8424b575f64a",
   "metadata": {},
   "source": [
    "GPS coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c5a67c4f-931a-4a9c-9947-d9f144e9f9ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1001224523</th>\n",
       "      <td>31.349944</td>\n",
       "      <td>-105.928990</td>\n",
       "      <td>airplane</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10030517043</th>\n",
       "      <td>43.123688</td>\n",
       "      <td>-77.623395</td>\n",
       "      <td>airplane</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             longitude    latitude     class\n",
       "1001224523   31.349944 -105.928990  airplane\n",
       "10030517043  43.123688  -77.623395  airplane"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read coordinates\n",
    "coordinates = pd.read_csv('data/photo2gps.txt', header=None, delimiter=' ', names=['longitude', 'latitude'], index_col=0)\n",
    "# Split index\n",
    "coordinates.index = coordinates.index.map(lambda s: tuple(s.split('/')))\n",
    "# Create class label\n",
    "coordinates['class'] = coordinates.index.get_level_values(0)\n",
    "# Set index\n",
    "coordinates.index = coordinates.index.get_level_values(1).map(lambda i: i[:-4])\n",
    "# Show examples\n",
    "coordinates.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "865a85e1-b2bb-4bef-a366-5fa248cca474",
   "metadata": {},
   "source": [
    "Class labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a194cfe7-0b72-484d-926b-de958d069b18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000481333    [bikini]\n",
       "10005385       [cactus]\n",
       "Name: class, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Group classes by image\n",
    "labels = coordinates.groupby(level=0)['class'].apply(list)\n",
    "# Get samples with only one label\n",
    "labels = labels[labels.apply(len) == 1]\n",
    "# Show example\n",
    "labels.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe828d9-2f62-4664-aa47-faa4d345ebe4",
   "metadata": {},
   "source": [
    "Class selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9fa11682-bf2f-4aaa-9b0a-95bc300bbc62",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Classes data directory\n",
    "data_directory = 'data/classes'\n",
    "# Select classes\n",
    "animal_classes = [\n",
    "    'ant', \n",
    "    'cow',\n",
    "    'deer',\n",
    "    'dolphin',  \n",
    "    'horse', \n",
    "    'jellyfish',\n",
    "    'lizard', \n",
    "    'lobster', \n",
    "    'sheep', \n",
    "    'whale'\n",
    "]\n",
    "transport_classes = [\n",
    "    'airplane',\n",
    "    'boat', \n",
    "    'canoe', \n",
    "    'chevrolet', \n",
    "    'corvette', \n",
    "    'ferrari', \n",
    "    'ford',\n",
    "    'helicopter',\n",
    "    'honda', \n",
    "    'jeep', \n",
    "    'locomotive', \n",
    "    'metro', \n",
    "    'mustang', \n",
    "    'ship', \n",
    "    'taxi', \n",
    "    'toyota', \n",
    "    'tractor',\n",
    "    'trailer', \n",
    "    'volkswagen', \n",
    "    'yacht'\n",
    "]\n",
    "sport_classes = [\n",
    "    'baseball', \n",
    "    'climbing',\n",
    "    'fishing', \n",
    "    'golf', \n",
    "    'hockey', \n",
    "    'ski', \n",
    "    'surfing'\n",
    "]\n",
    "structure_classes = [\n",
    "    'alcatraz',\n",
    "    'bridge', \n",
    "    'casino', \n",
    "    'castle', \n",
    "    'cemetery',\n",
    "    'greenhouse', \n",
    "    'monument', \n",
    "    'patio', \n",
    "    'pier', \n",
    "    'ranch', \n",
    "    'skyscraper', \n",
    "    'stadium', \n",
    "    'temple', \n",
    "    'tent', \n",
    "    'tower',\n",
    "    'tunnel'\n",
    "]\n",
    "nature_classes = [\n",
    "    'cactus', \n",
    "    'cave', \n",
    "    'cliff', \n",
    "    'falls', \n",
    "    'foliage', \n",
    "    'forest', \n",
    "    'lake',\n",
    "    'lightning', \n",
    "    'meadow', \n",
    "    'mountain', \n",
    "    'river', \n",
    "    'rocks', \n",
    "    'sea', \n",
    "    'snow',\n",
    "    'valley',\n",
    "]\n",
    "classes_select = [\n",
    "#     'alley', \n",
    "#     'aquarium', \n",
    "#     'asian',\n",
    "#     'autumn',\n",
    "#     'band',\n",
    "#     'beach', \n",
    "#     'bikini', \n",
    "#     'blond', \n",
    "#     'brick', \n",
    "#     'buildings',\n",
    "#     'carnival',\n",
    "#     'cattle' \n",
    "#     'cigar', \n",
    "#     'city',\n",
    "#     'coast', \n",
    "#     'disneyland', \n",
    "#     'fireworks', \n",
    "#     'fog', \n",
    "#     'fountain', \n",
    "#     'graffiti', \n",
    "#     'highway',\n",
    "#     'ipod', \n",
    "#     'pot', \n",
    "#     'museum',\n",
    "#     'nail',\n",
    "#     'ocean', \n",
    "#     'railroad', \n",
    "#     'scale', \n",
    "#     'sculpture',\n",
    "#     'shore', \n",
    "#     'storm', \n",
    "#     'wave', \n",
    "]\n",
    "# classes_select = animal_classes\n",
    "# classes_select = structure_classes\n",
    "# classes_select = sport_classes\n",
    "# classes_select = nature_classes\n",
    "classes_select = animal_classes + sport_classes + structure_classes + nature_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d7fb14bd-7f5e-44b2-b899-a82161a69a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes = len(classes_select)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ab2c4d0c-f437-47ef-bbf4-efe68c1befa8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('1031487825.jpg', 43159)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read images from directories\n",
    "photos, labels_ = zip(*[\n",
    "    (image, class_directory) for class_directory in classes_select\n",
    "    for image in os.listdir(os.path.join(data_directory, class_directory)) if image.endswith('jpg')\n",
    "])\n",
    "# Show example & amount of images\n",
    "photos[0], len(photos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "40fea50c-5940-4a9b-96bf-4bfa66e7765e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15157"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Photo ids with no suffix\n",
    "photos_no_suffix = [p[:-4] for p in photos]\n",
    "# Select photos for which labels are available\n",
    "photos_no_suffix = list(set(labels.index) & set(photos_no_suffix))\n",
    "# Select photos that have tags\n",
    "photos_no_suffix = list(set(tags.index) & set(photos_no_suffix))\n",
    "# Photo ids with sufix\n",
    "photos = [p + '.jpg' for p in photos_no_suffix]\n",
    "len(photos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9e824000-c45d-47de-a877-348b582cd588",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select tags and photos\n",
    "tags = tags.loc[photos_no_suffix]\n",
    "labels = labels.loc[photos_no_suffix]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bb45b9ef-4350-45af-9f8a-885145ce7217",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_tags(l, selection):\n",
    "    return [x for x in l if x not in selection]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c80df068-a662-4c03-87f3-5f9d4755f550",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "226061308                                             [water]\n",
       "1501113307    [underground, boat, pillar, cavern, bonneterre]\n",
       "Name: tags, dtype: object"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove tags that contain class label\n",
    "tags = tags.map(lambda t: remove_tags(t, classes_select))\n",
    "# Show example\n",
    "tags.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "184d8282-16c5-4a88-9dc9-65a6b5250c67",
   "metadata": {},
   "source": [
    "### Load word embeddings for context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c16e3bfe-149e-4004-9424-844c1477d7ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "# Dimension of glove to be used\n",
    "glove_dim = 300\n",
    "# Path to glove embeddings\n",
    "path_to_glove_file = f\"../glove/glove.6B.{glove_dim}d.txt\"\n",
    "# Make dictionary with glove embeddings\n",
    "embeddings_index = {}\n",
    "# Fill dictionary\n",
    "with open(path_to_glove_file) as f:\n",
    "    for line in f:\n",
    "        word, coefs = line.split(maxsplit=1)\n",
    "        coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "print(\"Found %s word vectors.\" % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09805fb8-b2c9-493f-93e5-b3dabb6e6a43",
   "metadata": {},
   "source": [
    "### Build context embeddings from context titles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9657ebc7-ac72-4423-8e05-4ba02dddafc5",
   "metadata": {},
   "source": [
    "Wanna construct context embeddings using SIF with word embeddings.\n",
    "\n",
    "$v_s = \\frac{1}{|s|} \\sum_{{v_w} \\in \\mathcal{S}} \\frac{a}{a + p(w)} v_w$\n",
    "\n",
    "With $a$ a parameter and $p(w)$ the estimated word frequency in the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a5bb2c51-9944-4eaf-b6de-c25a38a4cee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sif formula for a sentence as list of words\n",
    "def SIF(s, a, p):\n",
    "    return np.mean([embeddings_index.get(w, np.zeros(glove_dim, dtype=np.float32)) * a / (a + p[w]) for w in s], axis=0)\n",
    "\n",
    "# Normalization function for words\n",
    "def normalize(input_str):\n",
    "    nfkd_form = unicodedata.normalize('NFKD', input_str)\n",
    "    only_ascii = nfkd_form.encode('ASCII', 'ignore')\n",
    "    return re.sub(r'[0-9]', '', only_ascii.lower().decode('utf-8'))\n",
    "\n",
    "def preprocess_title(title):\n",
    "    return [normalize(w) for tag in title for word in re.split(r'[^\\w]', tag) for w in word.split('_')]\n",
    "\n",
    "def preprocess(image):\n",
    "    image = tf.keras.applications.mobilenet.preprocess_input(image)\n",
    "    image = tf.image.resize(image, (resize_shape[0], resize_shape[1]))\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "79871508-c706-4a68-8c30-c54597a9dd6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get word count from context titles\n",
    "words = [word for words in tags for word in words]\n",
    "# get unique words and counts\n",
    "uniques, counts = np.unique(words, return_counts=True)\n",
    "# frequencies dictionary\n",
    "frequencies = dict(zip(uniques, counts/counts.sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f57a33e1-3d5d-4fd6-876a-333886cae724",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tags</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>226061308</th>\n",
       "      <td>[water]</td>\n",
       "      <td>tower</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1501113307</th>\n",
       "      <td>[underground, boat, pillar, cavern, bonneterre]</td>\n",
       "      <td>lake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>448422155</th>\n",
       "      <td>[vermont, covered, brattleboro, windham, dumme...</td>\n",
       "      <td>bridge</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>341870914</th>\n",
       "      <td>[chicago, aquarium, illinois, shedd]</td>\n",
       "      <td>jellyfish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2109355749</th>\n",
       "      <td>[nyc, usa, ny, newyork, reflection, building, ...</td>\n",
       "      <td>skyscraper</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194955368</th>\n",
       "      <td>[signs, sign, oregon, no, fairview, passiveagr...</td>\n",
       "      <td>rocks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>401650530</th>\n",
       "      <td>[sunset, water, point, vanishingpoint, vanishi...</td>\n",
       "      <td>bridge</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111096353</th>\n",
       "      <td>[austin, flag, universityoftexas]</td>\n",
       "      <td>tower</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>861115696</th>\n",
       "      <td>[usa, sandwich, collegepark]</td>\n",
       "      <td>deer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>413093354</th>\n",
       "      <td>[windmill, texas, canvas, oil, land, top20texas]</td>\n",
       "      <td>ranch</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1039554741</th>\n",
       "      <td>[mountains, bike, bicycle, rockies, triangle, ...</td>\n",
       "      <td>valley</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>361953817</th>\n",
       "      <td>[birds, lago, orlando, florida, aves]</td>\n",
       "      <td>lake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>467825167</th>\n",
       "      <td>[alexandria, architecture, virginia, towers, c...</td>\n",
       "      <td>tower</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177329569</th>\n",
       "      <td>[chicago, stairs, moocards]</td>\n",
       "      <td>river</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>685761898</th>\n",
       "      <td>[railroad, scale, club, train, miniature, wood...</td>\n",
       "      <td>forest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>294841151</th>\n",
       "      <td>[seattle, plants, 2006, wa, seattleconservatory]</td>\n",
       "      <td>greenhouse</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298578228</th>\n",
       "      <td>[winter, fence, landscape, backyard, nebraska,...</td>\n",
       "      <td>snow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1516094763</th>\n",
       "      <td>[nc, northcarolina, hotels, capehatteras, brya...</td>\n",
       "      <td>surfing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203566476</th>\n",
       "      <td>[weather, racine, shoop]</td>\n",
       "      <td>golf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52682865</th>\n",
       "      <td>[california, me, geotagged, yosemitenationalpa...</td>\n",
       "      <td>snow</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                         tags       class\n",
       "226061308                                             [water]       tower\n",
       "1501113307    [underground, boat, pillar, cavern, bonneterre]        lake\n",
       "448422155   [vermont, covered, brattleboro, windham, dumme...      bridge\n",
       "341870914                [chicago, aquarium, illinois, shedd]   jellyfish\n",
       "2109355749  [nyc, usa, ny, newyork, reflection, building, ...  skyscraper\n",
       "194955368   [signs, sign, oregon, no, fairview, passiveagr...       rocks\n",
       "401650530   [sunset, water, point, vanishingpoint, vanishi...      bridge\n",
       "111096353                   [austin, flag, universityoftexas]       tower\n",
       "861115696                        [usa, sandwich, collegepark]        deer\n",
       "413093354    [windmill, texas, canvas, oil, land, top20texas]       ranch\n",
       "1039554741  [mountains, bike, bicycle, rockies, triangle, ...      valley\n",
       "361953817               [birds, lago, orlando, florida, aves]        lake\n",
       "467825167   [alexandria, architecture, virginia, towers, c...       tower\n",
       "177329569                         [chicago, stairs, moocards]       river\n",
       "685761898   [railroad, scale, club, train, miniature, wood...      forest\n",
       "294841151    [seattle, plants, 2006, wa, seattleconservatory]  greenhouse\n",
       "298578228   [winter, fence, landscape, backyard, nebraska,...        snow\n",
       "1516094763  [nc, northcarolina, hotels, capehatteras, brya...     surfing\n",
       "203566476                            [weather, racine, shoop]        golf\n",
       "52682865    [california, me, geotagged, yosemitenationalpa...        snow"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make datframe with classes and tags\n",
    "df = pd.concat([tags, labels.map(lambda l: l[0])], axis=1)\n",
    "# Remove no context words rows\n",
    "df = df.loc[df.tags.map(len) != 0]\n",
    "# Show example\n",
    "df.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52f5c441-2687-457a-b76a-90af66bb70ef",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Build dataset from generator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a12fa87-2083-48c3-a97d-9f9b452140b0",
   "metadata": {},
   "source": [
    "### Load images from folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "20c0d1ec-01ee-4316-a474-b771272bb2ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_from_id(item, image_id):\n",
    "    path = f'data/classes/{item}/{image_id}.jpg'\n",
    "    # return imageio.imread(path)\n",
    "    return tf.keras.utils.load_img(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f286b104-eab3-4042-8c06-1d8599c87795",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_generator(df):\n",
    "    def generator():\n",
    "        for PHOTO_ID, row in df.iterrows():\n",
    "            context_tags = row['tags']\n",
    "            item = row['class']\n",
    "            try:\n",
    "                x = get_image_from_id(item, PHOTO_ID)\n",
    "            except UnidentifiedImageError:\n",
    "                continue\n",
    "            x = tf.keras.preprocessing.image.img_to_array(x)\n",
    "            x = preprocess(x).numpy()\n",
    "            c = SIF(context_tags, a, frequencies)\n",
    "            y = (np.array(classes_select) == item).astype(np.int32)\n",
    "            yield (x, c), y\n",
    "    return generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5ef73720-fa07-4b51-989d-e3d60f072b0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tags</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>481326326</th>\n",
       "      <td>[sanfrancisco, california, usa, island, prison...</td>\n",
       "      <td>alcatraz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51087958</th>\n",
       "      <td>[2005, sanfrancisco, october, airshow, blueang...</td>\n",
       "      <td>alcatraz</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                        tags     class\n",
       "481326326  [sanfrancisco, california, usa, island, prison...  alcatraz\n",
       "51087958   [2005, sanfrancisco, october, airshow, blueang...  alcatraz"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train, val and test split\n",
    "def split_df(df):\n",
    "    train_idx = []\n",
    "    val_idx = []\n",
    "    test_idx = []\n",
    "    for g in df.groupby('class')['class']:\n",
    "        idx = g[1].sample(frac=1, random_state=42).index\n",
    "        train_idx.extend(idx[:int(0.8*idx.size)].to_list())\n",
    "        val_idx.extend(idx[int(0.8*idx.size) : int((0.8 * 0.5 + 0.5)* idx.size)])\n",
    "        test_idx.extend(idx[int((0.8 * 0.5 + 0.5)* idx.size) : ])\n",
    "    train_df = df.loc[train_idx]\n",
    "    val_df = df.loc[test_idx]\n",
    "    test_df = df.loc[test_idx]\n",
    "    return train_df, test_df, val_df\n",
    "\n",
    "# split in train test and val with stratisfied\n",
    "train_df, test_df, val_df = split_df(df)\n",
    "# show train example\n",
    "train_df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8ecc90b-1a91-49e9-91f2-990f0cbf4b56",
   "metadata": {},
   "source": [
    "### Storage strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6455bdf0-a174-462a-a4e5-9d0032664e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3b959b8f-f16a-4d57-92fd-7a1bddc90f1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/device:GPU:0']\n",
      "INFO:tensorflow:ParameterServerStrategy (CentralStorageStrategy if you are using a single machine) with compute_devices = ['/job:localhost/replica:0/task:0/device:GPU:0'], variable_device = '/job:localhost/replica:0/task:0/device:GPU:0'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-31 21:58:13.970986: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-01-31 21:58:15.782695: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /device:GPU:0 with 10228 MB memory:  -> device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:5e:00.0, compute capability: 6.1\n",
      "2022-01-31 21:58:15.838411: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 10228 MB memory:  -> device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:5e:00.0, compute capability: 6.1\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "def get_available_gpus():\n",
    "    local_device_protos = device_lib.list_local_devices()\n",
    "    return [x.name for x in local_device_protos if x.device_type == 'GPU']\n",
    "gpus = get_available_gpus()\n",
    "print(gpus)\n",
    "central_storage_strategy = tf.distribute.experimental.CentralStorageStrategy(compute_devices=[gpus[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3c9e3f53-b7ad-4970-aac4-d40bd7b68ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with central_storage_strategy.scope():\n",
    "    # SIF parameter\n",
    "    a = 10e-4\n",
    "    # Image scale parameter\n",
    "    image_scale = 1\n",
    "\n",
    "    # Shape parameters\n",
    "    resize_shape = (224 * image_scale, 224 * image_scale)\n",
    "    input_shape = (resize_shape[0], resize_shape[1], 3)\n",
    "\n",
    "    # Batch size\n",
    "    batch_size = 32\n",
    "\n",
    "    # Train dataset\n",
    "    train_ds = tf.data.Dataset.from_generator(\n",
    "        build_generator(train_df), \n",
    "        output_shapes=((input_shape, [glove_dim]), [n_classes]),\n",
    "        output_types=((tf.float32, tf.float32), tf.int32)\n",
    "    ).batch(batch_size, drop_remainder=True).cache()\n",
    "\n",
    "    # Validation dataset\n",
    "    val_ds = tf.data.Dataset.from_generator(\n",
    "        build_generator(val_df), \n",
    "        output_shapes=((input_shape, [glove_dim]), [n_classes]),\n",
    "        output_types=((tf.float32, tf.float32), tf.int32)\n",
    "    ).batch(batch_size, drop_remainder=True).cache()\n",
    "\n",
    "    # Test dataset\n",
    "    test_ds = tf.data.Dataset.from_generator(\n",
    "        build_generator(test_df), \n",
    "        output_shapes=((input_shape, [glove_dim]), [n_classes]),\n",
    "        output_types=((tf.float32, tf.float32), tf.int32)\n",
    "    ).batch(batch_size, drop_remainder=True).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d2293544-8541-47eb-8cde-69054a0fd624",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disable AutoShard.\n",
    "options = tf.data.Options()\n",
    "options.experimental_distribute.auto_shard_policy = tf.data.experimental.AutoShardPolicy.DATA\n",
    "options.experimental_optimization.apply_default_optimizations = False\n",
    "train_ds= train_ds.with_options(options)\n",
    "val_ds = val_ds.with_options(options)\n",
    "test_ds = test_ds.with_options(options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e7216fdd-c7b6-40e3-9277-ea66e9c39b82",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_23977/3392479804.py:2: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for (x, c), y in tqdm.tqdm_notebook(train_ds):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad097bae4db44636b68d67db1bee013e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 224, 224, 3) (32, 300) (32, 48)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-31 21:58:17.210146: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "for (x, c), y in tqdm.tqdm_notebook(train_ds):\n",
    "    batch_size = x.shape[0]\n",
    "    input_shape = tuple(x.shape[1:])\n",
    "    glove_dim = c.shape[-1]\n",
    "    n_classes = y.shape[-1]\n",
    "    break\n",
    "print(x.shape, c.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26db5fcb-eab0-4efc-b820-1c36801ed23e",
   "metadata": {},
   "source": [
    "### Network parameters & loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "689ed264-a4a0-4120-960e-ee0d0894bc30",
   "metadata": {},
   "outputs": [],
   "source": [
    "with central_storage_strategy.scope():\n",
    "    # Learning parameters\n",
    "    learning_rate = 10e-6/2\n",
    "    nu = 10e-6/2\n",
    "    epochs = 50\n",
    "    decays = 3\n",
    "    verbose = 1\n",
    "    rank_factor = 0.5\n",
    "\n",
    "    # ResNet50 trainable \n",
    "    trainable = False\n",
    "\n",
    "    # MLP parameters\n",
    "    n_layers = 3\n",
    "\n",
    "    # Loss function\n",
    "    loss_fn = tf.keras.losses.CategoricalCrossentropy()\n",
    "    # Metric function\n",
    "    metric_fn = tf.keras.metrics.CategoricalAccuracy()\n",
    "\n",
    "    # Weights for resnet\n",
    "    weights = 'imagenet'\n",
    "\n",
    "    # Early stopping\n",
    "    ES = tf.keras.callbacks.EarlyStopping('val_categorical_accuracy', patience=3)\n",
    "    # Reduce learning rate on plateau\n",
    "    # RLOP_lr = ReduceLROnPlateau(monitor=\"val_loss\", attributes=['learning_rate'], factor=0.5, patience=5, verbose=1)\n",
    "    # RLOP_lr_nu = ReduceLROnPlateau(monitor=\"val_loss\", attributes=['learning_rate', 'nu'], factor=0.5, patience=5, verbose=1)\n",
    "    RLOP = tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=5, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bc632bb5-deac-41a6-89f4-c3c2a4aec09f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_history(history, name):\n",
    "    # convert the history.history dict to a pandas DataFrame:     \n",
    "    hist_df = pd.DataFrame(history) if isinstance(history, dict) else pd.DataFrame(history.history)\n",
    "\n",
    "    # save to csv\n",
    "    hist_csv_file = f'{name}_history.csv'\n",
    "    with open(hist_csv_file, mode='w') as f:\n",
    "        hist_df.to_csv(f)\n",
    "        \n",
    "def history_list_to_dict(l):\n",
    "    return {k: itertools.chain.from_iterable([dic.history[k] for dic in l]) for k in l[0].history}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4ffffbfa-17ee-46ed-8ea3-570c686f393d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with central_storage_strategy.scope():\n",
    "    # Image net pretrained efficientnet\n",
    "    ImageNet = tf.keras.applications.ResNet50V2(\n",
    "        include_top=False,\n",
    "        weights=weights,\n",
    "        input_shape=input_shape,\n",
    "        pooling='max'\n",
    "    )\n",
    "    # Make efficientnet not trainable\n",
    "    for layer in ImageNet.layers:\n",
    "        layer.trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c11a3083-11d8-43a7-a8a5-da834ef10df3",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Additive context-aware MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "58d4a570-5aec-4b2b-830c-74822fe90b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with central_storage_strategy.scope():\n",
    "    # context model\n",
    "    context_inputs = context_hidden = tf.keras.layers.Input(shape=(glove_dim))\n",
    "    #Context hidden layers\n",
    "    for context_layer_idx in range(n_layers):\n",
    "        context_hiddens = tf.keras.layers.Dense(context_inputs.shape[-1], 'relu')(context_hidden)\n",
    "    # context output\n",
    "    context_outputs = context_hidden\n",
    "    # Context model build\n",
    "    context_model = tf.keras.models.Model(context_inputs, context_outputs)\n",
    "    # Image input layer\n",
    "    inputs = tf.keras.layers.Input(shape=input_shape)\n",
    "    # Context embedding input layer\n",
    "    contexts = tf.keras.layers.Input(shape=(glove_dim))\n",
    "    # Apply context model\n",
    "    contexts = context_model(contexts)\n",
    "    # ResNet 50 features\n",
    "    features = ImageNet(inputs)\n",
    "    # Get rank such that number of parameters are identical\n",
    "    add_rank = int(glove_dim + (features.shape[-1]**2)/(2*features.shape[-1] + glove_dim))\n",
    "    # Number of CA MLP layers\n",
    "    for layer_idx in range(n_layers):\n",
    "        con = tf.keras.layers.Dense(int(add_rank * rank_factor), 'relu')(contexts)\n",
    "        features = CADenseAdd(features.shape[-1], int(add_rank * rank_factor), activation='relu', use_bias=False)([features, contexts])\n",
    "    # Output layer\n",
    "    outputs = tf.keras.layers.Dense(n_classes, 'softmax')(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fd4fa136-d043-4497-b0e3-7a36db7ec552",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-31 22:00:46.784181: I tensorflow/stream_executor/cuda/cuda_dnn.cc:366] Loaded cuDNN version 8101\n",
      "2022-01-31 22:00:48.325372: I tensorflow/core/util/cuda_solvers.cc:179] Creating GpuSolver handles for stream 0x5603601e4100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "338/338 [==============================] - 573s 2s/step - loss: 3.8666 - categorical_accuracy: 0.0904 - val_loss: 3.6112 - val_categorical_accuracy: 0.1071\n",
      "Epoch 2/50\n",
      "338/338 [==============================] - 249s 738ms/step - loss: 3.6435 - categorical_accuracy: 0.1046 - val_loss: 3.4881 - val_categorical_accuracy: 0.1071\n",
      "Epoch 3/50\n",
      "338/338 [==============================] - 249s 738ms/step - loss: 3.5042 - categorical_accuracy: 0.1400 - val_loss: 3.4073 - val_categorical_accuracy: 0.1086\n",
      "Epoch 4/50\n",
      "338/338 [==============================] - 252s 745ms/step - loss: 3.3936 - categorical_accuracy: 0.1584 - val_loss: 3.3467 - val_categorical_accuracy: 0.1183\n",
      "Epoch 5/50\n",
      "338/338 [==============================] - 250s 740ms/step - loss: 3.3119 - categorical_accuracy: 0.1580 - val_loss: 3.2629 - val_categorical_accuracy: 0.1473\n",
      "Epoch 6/50\n",
      "338/338 [==============================] - 250s 740ms/step - loss: 3.2248 - categorical_accuracy: 0.1698 - val_loss: 3.1633 - val_categorical_accuracy: 0.1696\n",
      "Epoch 7/50\n",
      "338/338 [==============================] - 251s 742ms/step - loss: 3.1248 - categorical_accuracy: 0.1984 - val_loss: 3.0669 - val_categorical_accuracy: 0.1875\n",
      "Epoch 8/50\n",
      "338/338 [==============================] - 253s 748ms/step - loss: 3.0242 - categorical_accuracy: 0.2322 - val_loss: 2.9666 - val_categorical_accuracy: 0.2158\n",
      "Epoch 9/50\n",
      "338/338 [==============================] - 251s 742ms/step - loss: 2.9256 - categorical_accuracy: 0.2604 - val_loss: 2.8832 - val_categorical_accuracy: 0.2277\n",
      "Epoch 10/50\n",
      "338/338 [==============================] - 251s 743ms/step - loss: 2.8408 - categorical_accuracy: 0.2812 - val_loss: 2.8215 - val_categorical_accuracy: 0.2366\n",
      "Epoch 11/50\n",
      "338/338 [==============================] - 251s 742ms/step - loss: 2.7684 - categorical_accuracy: 0.2924 - val_loss: 2.7660 - val_categorical_accuracy: 0.2411\n",
      "Epoch 12/50\n",
      "338/338 [==============================] - 251s 744ms/step - loss: 2.7020 - categorical_accuracy: 0.2985 - val_loss: 2.7196 - val_categorical_accuracy: 0.2314\n",
      "Epoch 13/50\n",
      "338/338 [==============================] - 251s 742ms/step - loss: 2.6485 - categorical_accuracy: 0.3033 - val_loss: 2.6717 - val_categorical_accuracy: 0.2351\n",
      "Epoch 14/50\n",
      "338/338 [==============================] - 251s 742ms/step - loss: 2.6016 - categorical_accuracy: 0.3058 - val_loss: 2.5891 - val_categorical_accuracy: 0.2470\n",
      "Epoch 15/50\n",
      "338/338 [==============================] - 251s 743ms/step - loss: 2.5447 - categorical_accuracy: 0.3164 - val_loss: 2.4902 - val_categorical_accuracy: 0.2872\n",
      "Epoch 16/50\n",
      "338/338 [==============================] - 251s 743ms/step - loss: 2.4753 - categorical_accuracy: 0.3354 - val_loss: 2.4152 - val_categorical_accuracy: 0.3177\n",
      "Epoch 17/50\n",
      "338/338 [==============================] - 251s 744ms/step - loss: 2.4059 - categorical_accuracy: 0.3541 - val_loss: 2.3668 - val_categorical_accuracy: 0.3378\n",
      "Epoch 18/50\n",
      "338/338 [==============================] - 251s 744ms/step - loss: 2.3416 - categorical_accuracy: 0.3709 - val_loss: 2.3376 - val_categorical_accuracy: 0.3423\n",
      "Epoch 19/50\n",
      "338/338 [==============================] - 254s 750ms/step - loss: 2.2827 - categorical_accuracy: 0.3847 - val_loss: 2.3224 - val_categorical_accuracy: 0.3512\n",
      "Epoch 20/50\n",
      "338/338 [==============================] - 251s 743ms/step - loss: 2.2309 - categorical_accuracy: 0.3952 - val_loss: 2.3180 - val_categorical_accuracy: 0.3475\n",
      "Epoch 21/50\n",
      "338/338 [==============================] - 251s 743ms/step - loss: 2.1863 - categorical_accuracy: 0.4034 - val_loss: 2.3161 - val_categorical_accuracy: 0.3490\n",
      "Epoch 22/50\n",
      "338/338 [==============================] - 251s 744ms/step - loss: 2.1496 - categorical_accuracy: 0.4086 - val_loss: 2.3022 - val_categorical_accuracy: 0.3423\n",
      "Epoch 23/50\n",
      "338/338 [==============================] - 251s 743ms/step - loss: 2.1172 - categorical_accuracy: 0.4130 - val_loss: 2.2780 - val_categorical_accuracy: 0.3497\n",
      "Epoch 24/50\n",
      "338/338 [==============================] - 251s 743ms/step - loss: 2.0863 - categorical_accuracy: 0.4163 - val_loss: 2.2663 - val_categorical_accuracy: 0.3400\n",
      "Epoch 25/50\n",
      "338/338 [==============================] - 251s 743ms/step - loss: 2.0582 - categorical_accuracy: 0.4216 - val_loss: 2.2846 - val_categorical_accuracy: 0.3281\n",
      "Epoch 26/50\n",
      "338/338 [==============================] - 253s 750ms/step - loss: 2.0411 - categorical_accuracy: 0.4242 - val_loss: 2.2855 - val_categorical_accuracy: 0.3371\n",
      "Epoch 27/50\n",
      "338/338 [==============================] - 251s 744ms/step - loss: 2.0261 - categorical_accuracy: 0.4277 - val_loss: 2.2215 - val_categorical_accuracy: 0.3571\n",
      "Epoch 28/50\n",
      "338/338 [==============================] - 251s 743ms/step - loss: 1.9960 - categorical_accuracy: 0.4417 - val_loss: 2.1279 - val_categorical_accuracy: 0.3943\n",
      "Epoch 29/50\n",
      "338/338 [==============================] - 251s 743ms/step - loss: 1.9529 - categorical_accuracy: 0.4564 - val_loss: 2.0745 - val_categorical_accuracy: 0.4115\n",
      "Epoch 30/50\n",
      "338/338 [==============================] - 251s 743ms/step - loss: 1.9119 - categorical_accuracy: 0.4720 - val_loss: 2.0619 - val_categorical_accuracy: 0.4100\n",
      "Epoch 31/50\n",
      "338/338 [==============================] - 251s 743ms/step - loss: 1.8808 - categorical_accuracy: 0.4798 - val_loss: 2.0613 - val_categorical_accuracy: 0.4107\n",
      "Epoch 32/50\n",
      "338/338 [==============================] - 251s 743ms/step - loss: 1.8554 - categorical_accuracy: 0.4859 - val_loss: 2.0566 - val_categorical_accuracy: 0.4137\n",
      "Epoch 33/50\n",
      "338/338 [==============================] - 251s 744ms/step - loss: 1.8311 - categorical_accuracy: 0.4919 - val_loss: 2.0466 - val_categorical_accuracy: 0.4137\n",
      "Epoch 34/50\n",
      "338/338 [==============================] - 254s 750ms/step - loss: 1.8057 - categorical_accuracy: 0.4977 - val_loss: 2.0278 - val_categorical_accuracy: 0.4196\n",
      "Epoch 35/50\n",
      "338/338 [==============================] - 251s 743ms/step - loss: 1.7796 - categorical_accuracy: 0.5025 - val_loss: 2.0075 - val_categorical_accuracy: 0.4308\n",
      "Epoch 36/50\n",
      "338/338 [==============================] - 251s 744ms/step - loss: 1.7533 - categorical_accuracy: 0.5067 - val_loss: 1.9946 - val_categorical_accuracy: 0.4286\n",
      "Epoch 37/50\n",
      "338/338 [==============================] - 254s 751ms/step - loss: 1.7309 - categorical_accuracy: 0.5086 - val_loss: 1.9982 - val_categorical_accuracy: 0.4271\n",
      "Epoch 38/50\n",
      "338/338 [==============================] - 251s 744ms/step - loss: 1.7143 - categorical_accuracy: 0.5123 - val_loss: 2.0129 - val_categorical_accuracy: 0.4249\n",
      "Epoch 39/50\n",
      "338/338 [==============================] - 252s 744ms/step - loss: 1.7029 - categorical_accuracy: 0.5160 - val_loss: 2.0260 - val_categorical_accuracy: 0.4189\n",
      "Epoch 40/50\n",
      "338/338 [==============================] - 251s 744ms/step - loss: 1.7033 - categorical_accuracy: 0.5121 - val_loss: 1.9991 - val_categorical_accuracy: 0.4271\n",
      "Epoch 41/50\n",
      "338/338 [==============================] - 251s 743ms/step - loss: 1.6892 - categorical_accuracy: 0.5202 - val_loss: 1.9525 - val_categorical_accuracy: 0.4442\n",
      "Epoch 42/50\n",
      "338/338 [==============================] - 251s 744ms/step - loss: 1.6684 - categorical_accuracy: 0.5263 - val_loss: 1.9171 - val_categorical_accuracy: 0.4583\n",
      "Epoch 43/50\n",
      "338/338 [==============================] - 252s 744ms/step - loss: 1.6454 - categorical_accuracy: 0.5352 - val_loss: 1.9108 - val_categorical_accuracy: 0.4531\n",
      "Epoch 44/50\n",
      "338/338 [==============================] - 252s 745ms/step - loss: 1.6271 - categorical_accuracy: 0.5405 - val_loss: 1.9134 - val_categorical_accuracy: 0.4472\n",
      "Epoch 45/50\n",
      "338/338 [==============================] - 251s 744ms/step - loss: 1.6114 - categorical_accuracy: 0.5439 - val_loss: 1.9070 - val_categorical_accuracy: 0.4509\n",
      "Epoch 46/50\n",
      "338/338 [==============================] - 252s 745ms/step - loss: 1.5937 - categorical_accuracy: 0.5459 - val_loss: 1.8885 - val_categorical_accuracy: 0.4598\n",
      "Epoch 47/50\n",
      "338/338 [==============================] - 251s 744ms/step - loss: 1.5725 - categorical_accuracy: 0.5499 - val_loss: 1.8658 - val_categorical_accuracy: 0.4643\n",
      "Epoch 48/50\n",
      "338/338 [==============================] - 252s 744ms/step - loss: 1.5469 - categorical_accuracy: 0.5537 - val_loss: 1.8477 - val_categorical_accuracy: 0.4710\n",
      "Epoch 49/50\n",
      "338/338 [==============================] - 252s 745ms/step - loss: 1.5188 - categorical_accuracy: 0.5645 - val_loss: 1.8415 - val_categorical_accuracy: 0.4777\n",
      "Epoch 50/50\n",
      "338/338 [==============================] - 252s 745ms/step - loss: 1.4910 - categorical_accuracy: 0.5700 - val_loss: 1.8476 - val_categorical_accuracy: 0.4814\n",
      "Epoch 1/50\n",
      "338/338 [==============================] - 263s 751ms/step - loss: 1.4056 - categorical_accuracy: 0.5838 - val_loss: 1.7249 - val_categorical_accuracy: 0.5149\n",
      "Epoch 2/50\n",
      "338/338 [==============================] - 251s 744ms/step - loss: 1.3472 - categorical_accuracy: 0.6151 - val_loss: 1.7086 - val_categorical_accuracy: 0.5156\n",
      "Epoch 3/50\n",
      "338/338 [==============================] - 251s 743ms/step - loss: 1.3266 - categorical_accuracy: 0.6191 - val_loss: 1.6997 - val_categorical_accuracy: 0.5186\n",
      "Epoch 4/50\n",
      "338/338 [==============================] - 251s 744ms/step - loss: 1.3115 - categorical_accuracy: 0.6232 - val_loss: 1.6930 - val_categorical_accuracy: 0.5208\n",
      "Epoch 5/50\n",
      "338/338 [==============================] - 251s 742ms/step - loss: 1.2985 - categorical_accuracy: 0.6271 - val_loss: 1.6867 - val_categorical_accuracy: 0.5231\n",
      "Epoch 6/50\n",
      "338/338 [==============================] - 251s 743ms/step - loss: 1.2864 - categorical_accuracy: 0.6305 - val_loss: 1.6808 - val_categorical_accuracy: 0.5275\n",
      "Epoch 7/50\n",
      "338/338 [==============================] - 251s 743ms/step - loss: 1.2749 - categorical_accuracy: 0.6330 - val_loss: 1.6752 - val_categorical_accuracy: 0.5275\n",
      "Epoch 8/50\n",
      "338/338 [==============================] - 251s 743ms/step - loss: 1.2640 - categorical_accuracy: 0.6368 - val_loss: 1.6699 - val_categorical_accuracy: 0.5290\n",
      "Epoch 9/50\n",
      "338/338 [==============================] - 251s 744ms/step - loss: 1.2534 - categorical_accuracy: 0.6399 - val_loss: 1.6649 - val_categorical_accuracy: 0.5342\n",
      "Epoch 10/50\n",
      "338/338 [==============================] - 252s 745ms/step - loss: 1.2434 - categorical_accuracy: 0.6419 - val_loss: 1.6600 - val_categorical_accuracy: 0.5365\n",
      "Epoch 11/50\n",
      "338/338 [==============================] - 252s 745ms/step - loss: 1.2338 - categorical_accuracy: 0.6457 - val_loss: 1.6553 - val_categorical_accuracy: 0.5372\n",
      "Epoch 12/50\n",
      "338/338 [==============================] - 251s 743ms/step - loss: 1.2245 - categorical_accuracy: 0.6477 - val_loss: 1.6508 - val_categorical_accuracy: 0.5379\n",
      "Epoch 13/50\n",
      "338/338 [==============================] - 251s 744ms/step - loss: 1.2155 - categorical_accuracy: 0.6499 - val_loss: 1.6465 - val_categorical_accuracy: 0.5417\n",
      "Epoch 14/50\n",
      "338/338 [==============================] - 251s 744ms/step - loss: 1.2066 - categorical_accuracy: 0.6515 - val_loss: 1.6424 - val_categorical_accuracy: 0.5409\n",
      "Epoch 15/50\n",
      "338/338 [==============================] - 251s 744ms/step - loss: 1.1982 - categorical_accuracy: 0.6537 - val_loss: 1.6385 - val_categorical_accuracy: 0.5439\n",
      "Epoch 16/50\n",
      "338/338 [==============================] - 251s 744ms/step - loss: 1.1898 - categorical_accuracy: 0.6562 - val_loss: 1.6345 - val_categorical_accuracy: 0.5439\n",
      "Epoch 17/50\n",
      "338/338 [==============================] - 251s 744ms/step - loss: 1.1817 - categorical_accuracy: 0.6589 - val_loss: 1.6306 - val_categorical_accuracy: 0.5454\n",
      "Epoch 18/50\n",
      "338/338 [==============================] - 251s 743ms/step - loss: 1.1730 - categorical_accuracy: 0.6614 - val_loss: 1.6265 - val_categorical_accuracy: 0.5432\n",
      "Epoch 19/50\n",
      "338/338 [==============================] - 251s 743ms/step - loss: 1.1652 - categorical_accuracy: 0.6629 - val_loss: 1.6227 - val_categorical_accuracy: 0.5461\n",
      "Epoch 20/50\n",
      "338/338 [==============================] - 251s 744ms/step - loss: 1.1573 - categorical_accuracy: 0.6653 - val_loss: 1.6185 - val_categorical_accuracy: 0.5491\n",
      "Epoch 21/50\n",
      "338/338 [==============================] - 252s 746ms/step - loss: 1.1492 - categorical_accuracy: 0.6680 - val_loss: 1.6145 - val_categorical_accuracy: 0.5506\n",
      "Epoch 22/50\n",
      "338/338 [==============================] - 254s 753ms/step - loss: 1.1410 - categorical_accuracy: 0.6704 - val_loss: 1.6103 - val_categorical_accuracy: 0.5521\n",
      "Epoch 23/50\n",
      "338/338 [==============================] - 252s 746ms/step - loss: 1.1326 - categorical_accuracy: 0.6728 - val_loss: 1.6065 - val_categorical_accuracy: 0.5513\n",
      "Epoch 24/50\n",
      "338/338 [==============================] - 252s 746ms/step - loss: 1.1240 - categorical_accuracy: 0.6746 - val_loss: 1.6027 - val_categorical_accuracy: 0.5528\n",
      "Epoch 25/50\n",
      "338/338 [==============================] - 252s 746ms/step - loss: 1.1152 - categorical_accuracy: 0.6760 - val_loss: 1.5992 - val_categorical_accuracy: 0.5558\n",
      "Epoch 26/50\n",
      "338/338 [==============================] - 252s 745ms/step - loss: 1.1065 - categorical_accuracy: 0.6786 - val_loss: 1.5957 - val_categorical_accuracy: 0.5588\n",
      "Epoch 27/50\n",
      "338/338 [==============================] - 252s 747ms/step - loss: 1.0977 - categorical_accuracy: 0.6814 - val_loss: 1.5925 - val_categorical_accuracy: 0.5580\n",
      "Epoch 28/50\n",
      "338/338 [==============================] - 252s 745ms/step - loss: 1.0888 - categorical_accuracy: 0.6837 - val_loss: 1.5896 - val_categorical_accuracy: 0.5588\n",
      "Epoch 29/50\n",
      "338/338 [==============================] - 251s 742ms/step - loss: 1.0797 - categorical_accuracy: 0.6876 - val_loss: 1.5870 - val_categorical_accuracy: 0.5595\n",
      "Epoch 30/50\n",
      "338/338 [==============================] - 251s 744ms/step - loss: 1.0707 - categorical_accuracy: 0.6906 - val_loss: 1.5846 - val_categorical_accuracy: 0.5580\n",
      "Epoch 31/50\n",
      "338/338 [==============================] - 251s 743ms/step - loss: 1.0615 - categorical_accuracy: 0.6936 - val_loss: 1.5826 - val_categorical_accuracy: 0.5588\n",
      "Epoch 32/50\n",
      "338/338 [==============================] - 251s 743ms/step - loss: 1.0525 - categorical_accuracy: 0.6968 - val_loss: 1.5807 - val_categorical_accuracy: 0.5603\n",
      "Epoch 33/50\n",
      "338/338 [==============================] - 251s 743ms/step - loss: 1.0430 - categorical_accuracy: 0.7005 - val_loss: 1.5794 - val_categorical_accuracy: 0.5618\n",
      "Epoch 34/50\n",
      "338/338 [==============================] - 251s 743ms/step - loss: 1.0337 - categorical_accuracy: 0.7028 - val_loss: 1.5785 - val_categorical_accuracy: 0.5618\n",
      "Epoch 35/50\n",
      "338/338 [==============================] - 251s 743ms/step - loss: 1.0245 - categorical_accuracy: 0.7053 - val_loss: 1.5779 - val_categorical_accuracy: 0.5610\n",
      "Epoch 36/50\n",
      "338/338 [==============================] - 251s 744ms/step - loss: 1.0151 - categorical_accuracy: 0.7080 - val_loss: 1.5776 - val_categorical_accuracy: 0.5588\n",
      "Epoch 37/50\n",
      "338/338 [==============================] - 251s 744ms/step - loss: 1.0059 - categorical_accuracy: 0.7115 - val_loss: 1.5776 - val_categorical_accuracy: 0.5588\n",
      "Epoch 38/50\n",
      "338/338 [==============================] - 252s 746ms/step - loss: 0.9966 - categorical_accuracy: 0.7144 - val_loss: 1.5778 - val_categorical_accuracy: 0.5618\n",
      "Epoch 39/50\n",
      "338/338 [==============================] - 252s 744ms/step - loss: 0.9875 - categorical_accuracy: 0.7171 - val_loss: 1.5784 - val_categorical_accuracy: 0.5625\n",
      "Epoch 40/50\n",
      "338/338 [==============================] - 252s 745ms/step - loss: 0.9783 - categorical_accuracy: 0.7206 - val_loss: 1.5791 - val_categorical_accuracy: 0.5632\n",
      "Epoch 41/50\n",
      "338/338 [==============================] - 252s 744ms/step - loss: 0.9691 - categorical_accuracy: 0.7242 - val_loss: 1.5800 - val_categorical_accuracy: 0.5640\n",
      "Epoch 42/50\n",
      "338/338 [==============================] - 252s 746ms/step - loss: 0.9599 - categorical_accuracy: 0.7268 - val_loss: 1.5811 - val_categorical_accuracy: 0.5647\n",
      "Epoch 43/50\n",
      "338/338 [==============================] - 252s 745ms/step - loss: 0.9508 - categorical_accuracy: 0.7294 - val_loss: 1.5823 - val_categorical_accuracy: 0.5640\n",
      "Epoch 44/50\n",
      "338/338 [==============================] - 252s 745ms/step - loss: 0.9417 - categorical_accuracy: 0.7321 - val_loss: 1.5837 - val_categorical_accuracy: 0.5655\n",
      "Epoch 45/50\n",
      "338/338 [==============================] - 252s 745ms/step - loss: 0.9327 - categorical_accuracy: 0.7359 - val_loss: 1.5850 - val_categorical_accuracy: 0.5655\n",
      "Epoch 46/50\n",
      "338/338 [==============================] - 252s 744ms/step - loss: 0.9236 - categorical_accuracy: 0.7390 - val_loss: 1.5865 - val_categorical_accuracy: 0.5662\n",
      "Epoch 47/50\n",
      "338/338 [==============================] - 253s 747ms/step - loss: 0.9147 - categorical_accuracy: 0.7415 - val_loss: 1.5882 - val_categorical_accuracy: 0.5662\n",
      "Epoch 48/50\n",
      "338/338 [==============================] - 252s 745ms/step - loss: 0.9057 - categorical_accuracy: 0.7449 - val_loss: 1.5900 - val_categorical_accuracy: 0.5640\n",
      "Epoch 49/50\n",
      "338/338 [==============================] - 252s 745ms/step - loss: 0.8968 - categorical_accuracy: 0.7473 - val_loss: 1.5918 - val_categorical_accuracy: 0.5640\n",
      "Epoch 50/50\n",
      "338/338 [==============================] - 252s 746ms/step - loss: 0.8880 - categorical_accuracy: 0.7512 - val_loss: 1.5937 - val_categorical_accuracy: 0.5625\n",
      "Epoch 1/50\n",
      "338/338 [==============================] - 264s 752ms/step - loss: 0.8270 - categorical_accuracy: 0.7484 - val_loss: 1.5444 - val_categorical_accuracy: 0.5774\n",
      "Epoch 2/50\n",
      "338/338 [==============================] - 253s 748ms/step - loss: 0.8024 - categorical_accuracy: 0.7795 - val_loss: 1.5397 - val_categorical_accuracy: 0.5789\n",
      "Epoch 3/50\n",
      "338/338 [==============================] - 254s 752ms/step - loss: 0.7949 - categorical_accuracy: 0.7811 - val_loss: 1.5383 - val_categorical_accuracy: 0.5789\n",
      "Epoch 4/50\n",
      "338/338 [==============================] - 253s 748ms/step - loss: 0.7890 - categorical_accuracy: 0.7829 - val_loss: 1.5379 - val_categorical_accuracy: 0.5818\n",
      "Epoch 5/50\n",
      "338/338 [==============================] - 253s 748ms/step - loss: 0.7836 - categorical_accuracy: 0.7836 - val_loss: 1.5379 - val_categorical_accuracy: 0.5826\n",
      "Epoch 6/50\n",
      "338/338 [==============================] - 253s 749ms/step - loss: 0.7785 - categorical_accuracy: 0.7855 - val_loss: 1.5380 - val_categorical_accuracy: 0.5818\n",
      "Epoch 7/50\n",
      "338/338 [==============================] - 253s 748ms/step - loss: 0.7735 - categorical_accuracy: 0.7873 - val_loss: 1.5382 - val_categorical_accuracy: 0.5811\n",
      "Epoch 8/50\n",
      "338/338 [==============================] - 252s 746ms/step - loss: 0.7686 - categorical_accuracy: 0.7888 - val_loss: 1.5383 - val_categorical_accuracy: 0.5811\n",
      "Epoch 9/50\n",
      "338/338 [==============================] - 253s 747ms/step - loss: 0.7637 - categorical_accuracy: 0.7897 - val_loss: 1.5385 - val_categorical_accuracy: 0.5804\n",
      "Epoch 10/50\n",
      "338/338 [==============================] - 255s 753ms/step - loss: 0.7589 - categorical_accuracy: 0.7911 - val_loss: 1.5387 - val_categorical_accuracy: 0.5804\n",
      "Epoch 11/50\n",
      "338/338 [==============================] - 252s 747ms/step - loss: 0.7541 - categorical_accuracy: 0.7923 - val_loss: 1.5389 - val_categorical_accuracy: 0.5804\n",
      "Epoch 12/50\n",
      "338/338 [==============================] - 252s 746ms/step - loss: 0.7494 - categorical_accuracy: 0.7943 - val_loss: 1.5392 - val_categorical_accuracy: 0.5804\n",
      "Epoch 13/50\n",
      "338/338 [==============================] - 253s 747ms/step - loss: 0.7447 - categorical_accuracy: 0.7953 - val_loss: 1.5394 - val_categorical_accuracy: 0.5804\n",
      "Epoch 14/50\n",
      "338/338 [==============================] - 252s 746ms/step - loss: 0.7401 - categorical_accuracy: 0.7970 - val_loss: 1.5397 - val_categorical_accuracy: 0.5811\n",
      "Epoch 15/50\n",
      "338/338 [==============================] - 252s 747ms/step - loss: 0.7355 - categorical_accuracy: 0.7983 - val_loss: 1.5400 - val_categorical_accuracy: 0.5804\n",
      "Epoch 16/50\n",
      "338/338 [==============================] - 252s 747ms/step - loss: 0.7309 - categorical_accuracy: 0.7999 - val_loss: 1.5404 - val_categorical_accuracy: 0.5804\n",
      "Epoch 17/50\n",
      "338/338 [==============================] - 252s 747ms/step - loss: 0.7264 - categorical_accuracy: 0.8017 - val_loss: 1.5408 - val_categorical_accuracy: 0.5796\n",
      "Epoch 18/50\n",
      "338/338 [==============================] - 253s 747ms/step - loss: 0.7220 - categorical_accuracy: 0.8031 - val_loss: 1.5412 - val_categorical_accuracy: 0.5826\n",
      "Epoch 19/50\n",
      "338/338 [==============================] - 252s 746ms/step - loss: 0.7175 - categorical_accuracy: 0.8041 - val_loss: 1.5416 - val_categorical_accuracy: 0.5826\n",
      "Epoch 20/50\n",
      "338/338 [==============================] - 252s 746ms/step - loss: 0.7132 - categorical_accuracy: 0.8059 - val_loss: 1.5420 - val_categorical_accuracy: 0.5833\n",
      "Epoch 21/50\n",
      "338/338 [==============================] - 252s 747ms/step - loss: 0.7088 - categorical_accuracy: 0.8073 - val_loss: 1.5425 - val_categorical_accuracy: 0.5833\n",
      "Epoch 22/50\n",
      "338/338 [==============================] - 253s 748ms/step - loss: 0.7045 - categorical_accuracy: 0.8091 - val_loss: 1.5429 - val_categorical_accuracy: 0.5833\n",
      "Epoch 23/50\n",
      "338/338 [==============================] - 253s 748ms/step - loss: 0.7002 - categorical_accuracy: 0.8103 - val_loss: 1.5435 - val_categorical_accuracy: 0.5841\n",
      "Epoch 24/50\n",
      "338/338 [==============================] - 253s 749ms/step - loss: 0.6960 - categorical_accuracy: 0.8116 - val_loss: 1.5439 - val_categorical_accuracy: 0.5848\n",
      "Epoch 25/50\n",
      "338/338 [==============================] - 253s 747ms/step - loss: 0.6917 - categorical_accuracy: 0.8127 - val_loss: 1.5445 - val_categorical_accuracy: 0.5848\n",
      "Epoch 26/50\n",
      "338/338 [==============================] - 253s 748ms/step - loss: 0.6875 - categorical_accuracy: 0.8143 - val_loss: 1.5451 - val_categorical_accuracy: 0.5848\n",
      "Epoch 27/50\n",
      "338/338 [==============================] - 253s 747ms/step - loss: 0.6833 - categorical_accuracy: 0.8158 - val_loss: 1.5456 - val_categorical_accuracy: 0.5833\n",
      "Epoch 28/50\n",
      "338/338 [==============================] - 253s 748ms/step - loss: 0.6791 - categorical_accuracy: 0.8171 - val_loss: 1.5463 - val_categorical_accuracy: 0.5833\n",
      "Epoch 29/50\n",
      "338/338 [==============================] - 252s 747ms/step - loss: 0.6750 - categorical_accuracy: 0.8178 - val_loss: 1.5469 - val_categorical_accuracy: 0.5826\n",
      "Epoch 30/50\n",
      "338/338 [==============================] - 252s 747ms/step - loss: 0.6708 - categorical_accuracy: 0.8189 - val_loss: 1.5477 - val_categorical_accuracy: 0.5818\n",
      "Epoch 31/50\n",
      "338/338 [==============================] - 252s 747ms/step - loss: 0.6667 - categorical_accuracy: 0.8196 - val_loss: 1.5485 - val_categorical_accuracy: 0.5826\n",
      "Epoch 32/50\n",
      "338/338 [==============================] - 253s 748ms/step - loss: 0.6626 - categorical_accuracy: 0.8209 - val_loss: 1.5492 - val_categorical_accuracy: 0.5826\n",
      "Epoch 33/50\n",
      "338/338 [==============================] - 252s 747ms/step - loss: 0.6585 - categorical_accuracy: 0.8220 - val_loss: 1.5499 - val_categorical_accuracy: 0.5826\n",
      "Epoch 34/50\n",
      "338/338 [==============================] - 252s 747ms/step - loss: 0.6545 - categorical_accuracy: 0.8237 - val_loss: 1.5507 - val_categorical_accuracy: 0.5833\n",
      "Epoch 35/50\n",
      "338/338 [==============================] - 252s 747ms/step - loss: 0.6504 - categorical_accuracy: 0.8247 - val_loss: 1.5515 - val_categorical_accuracy: 0.5841\n",
      "Epoch 36/50\n",
      "338/338 [==============================] - 252s 747ms/step - loss: 0.6464 - categorical_accuracy: 0.8263 - val_loss: 1.5524 - val_categorical_accuracy: 0.5841\n",
      "Epoch 37/50\n",
      "338/338 [==============================] - 252s 747ms/step - loss: 0.6424 - categorical_accuracy: 0.8276 - val_loss: 1.5533 - val_categorical_accuracy: 0.5848\n",
      "Epoch 38/50\n",
      "338/338 [==============================] - 252s 746ms/step - loss: 0.6384 - categorical_accuracy: 0.8288 - val_loss: 1.5542 - val_categorical_accuracy: 0.5841\n",
      "Epoch 39/50\n",
      "338/338 [==============================] - 252s 745ms/step - loss: 0.6344 - categorical_accuracy: 0.8300 - val_loss: 1.5551 - val_categorical_accuracy: 0.5841\n",
      "Epoch 40/50\n",
      "338/338 [==============================] - 252s 746ms/step - loss: 0.6305 - categorical_accuracy: 0.8311 - val_loss: 1.5561 - val_categorical_accuracy: 0.5848\n",
      "Epoch 41/50\n",
      "338/338 [==============================] - 252s 746ms/step - loss: 0.6265 - categorical_accuracy: 0.8321 - val_loss: 1.5570 - val_categorical_accuracy: 0.5848\n",
      "Epoch 42/50\n",
      "338/338 [==============================] - 254s 753ms/step - loss: 0.6226 - categorical_accuracy: 0.8336 - val_loss: 1.5580 - val_categorical_accuracy: 0.5863\n",
      "Epoch 43/50\n",
      "338/338 [==============================] - 253s 750ms/step - loss: 0.6187 - categorical_accuracy: 0.8348 - val_loss: 1.5590 - val_categorical_accuracy: 0.5871\n",
      "Epoch 44/50\n",
      "338/338 [==============================] - 253s 748ms/step - loss: 0.6148 - categorical_accuracy: 0.8360 - val_loss: 1.5600 - val_categorical_accuracy: 0.5878\n",
      "Epoch 45/50\n",
      "338/338 [==============================] - 252s 747ms/step - loss: 0.6109 - categorical_accuracy: 0.8372 - val_loss: 1.5611 - val_categorical_accuracy: 0.5871\n",
      "Epoch 46/50\n",
      "338/338 [==============================] - 253s 747ms/step - loss: 0.6071 - categorical_accuracy: 0.8379 - val_loss: 1.5622 - val_categorical_accuracy: 0.5871\n",
      "Epoch 47/50\n",
      "338/338 [==============================] - 253s 748ms/step - loss: 0.6033 - categorical_accuracy: 0.8393 - val_loss: 1.5632 - val_categorical_accuracy: 0.5878\n",
      "Epoch 48/50\n",
      "338/338 [==============================] - 253s 748ms/step - loss: 0.5994 - categorical_accuracy: 0.8405 - val_loss: 1.5643 - val_categorical_accuracy: 0.5885\n",
      "Epoch 49/50\n",
      "338/338 [==============================] - 252s 747ms/step - loss: 0.5956 - categorical_accuracy: 0.8415 - val_loss: 1.5655 - val_categorical_accuracy: 0.5871\n",
      "Epoch 50/50\n",
      "338/338 [==============================] - 253s 748ms/step - loss: 0.5917 - categorical_accuracy: 0.8426 - val_loss: 1.5666 - val_categorical_accuracy: 0.5863\n"
     ]
    }
   ],
   "source": [
    "add_history_list = []\n",
    "with central_storage_strategy.scope():\n",
    "    # Build model\n",
    "    add_model = tf.keras.models.Model((inputs, contexts), outputs)\n",
    "    # Wrop model\n",
    "    add_model = wrap_model(add_model)\n",
    "    for decay in range(decays):\n",
    "        # Build optimizer\n",
    "        add_optimizer = SVDAdam(add_model, context_model, learning_rate/2**decay, learning_rate/2**decay)\n",
    "        # Compile model\n",
    "        add_model.compile(add_optimizer, tf.keras.losses.CategoricalCrossentropy(), metric_fn)\n",
    "        # train\n",
    "        add_history_list.append(\n",
    "            add_model.fit(train_ds, epochs=epochs, verbose=verbose, validation_data=val_ds, callbacks=[]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "80c471ff-fc10-4165-9d01-733a930915af",
   "metadata": {},
   "outputs": [],
   "source": [
    "add_history = history_list_to_dict(add_history_list)\n",
    "save_history(add_history, f'histories/context_aware_{n_layers}_add')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a532fd3-8970-4105-a11f-43e75ad1e07d",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Multiplicative context-aware MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d839c42a-c071-4aa5-a350-9ccd91ba53b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with central_storage_strategy.scope():\n",
    "    # Image input layer\n",
    "    inputs = tf.keras.layers.Input(shape=input_shape)\n",
    "    # Context embedding input layer\n",
    "    contexts = tf.keras.layers.Input(shape=(glove_dim))\n",
    "    # ResNet 50 features\n",
    "    features = ImageNet(inputs)\n",
    "    # Get rank such that number of parameters are identical\n",
    "    mul_rank = int(features.shape[-1] * (features.shape[-1] + glove_dim)/(2*features.shape[-1] + glove_dim))\n",
    "    # Number of CA MLP layers\n",
    "    for layer_idx in range(n_layers):\n",
    "        features = CADenseMul(features.shape[-1], int(mul_rank * rank_factor), activation='elu', use_bias=False)([features, contexts])\n",
    "    # Output layer\n",
    "    outputs = tf.keras.layers.Dense(n_classes, 'softmax')(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f1c4bc1-cbbb-4b2a-a956-85b7f16ef0a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "mul_history_list = []\n",
    "with central_storage_strategy.scope():\n",
    "    # Build model\n",
    "    mul_model = tf.keras.models.Model((inputs, contexts), outputs)\n",
    "    # Wrop model\n",
    "    mul_model = wrap_model(mul_model)\n",
    "    for decay in range(decays):\n",
    "        # Build optimizer\n",
    "        mul_optimizer = SVDAdam(mul_model, None, learning_rate/2**decay, learning_rate/2**decay)\n",
    "        # Compile model\n",
    "        mul_model.compile(mul_optimizer, tf.keras.losses.CategoricalCrossentropy(), metric_fn)\n",
    "        # train\n",
    "        mul_history_list.append(\n",
    "            mul_model.fit(train_ds, epochs=epochs, verbose=verbose, validation_data=val_ds, callbacks=[]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29bb2c48-0678-49d8-82f7-c7df12ed7423",
   "metadata": {},
   "outputs": [],
   "source": [
    "mul_history = history_list_to_dict(mul_history_list)\n",
    "save_history(mul_history, f'histories/context_aware_{n_layers}_mul')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f24b1b-c0fe-499f-ac43-c2c40754b38a",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Regular MLP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e6cc4d-89a8-42b9-b9fe-25fdfdf0deaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "with central_storage_strategy.scope():\n",
    "    # Image input layer\n",
    "    inputs = tf.keras.layers.Input(shape=input_shape)\n",
    "    # Context embedding input layer\n",
    "    contexts = tf.keras.layers.Input(shape=(glove_dim))\n",
    "    # ResNet 50 features\n",
    "    features = ImageNet(inputs)\n",
    "    # Number of MLP layers\n",
    "    for layer_idx in range(n_layers):\n",
    "        features = tf.keras.layers.Dense(features.shape[-1], 'elu')(features)\n",
    "    # Output layer\n",
    "    outputs = tf.keras.layers.Dense(n_classes, 'softmax')(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a924941f-b1d2-46c7-adc3-3d9185379f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_history_list = []\n",
    "with central_storage_strategy.scope():\n",
    "    # Build model\n",
    "    reg_model = tf.keras.models.Model((inputs, contexts), outputs)\n",
    "    for decay in range(decays):\n",
    "        # Build optimizer\n",
    "        reg_optimizer = tf.keras.optimizers.Adam(learning_rate/2**decay)\n",
    "        # Compile model\n",
    "        reg_model.compile(reg_optimizer, tf.keras.losses.CategoricalCrossentropy(), metric_fn)\n",
    "        # train\n",
    "        reg_history_list.append(\n",
    "            reg_model.fit(train_ds, epochs=epochs, verbose=verbose, validation_data=val_ds, callbacks=[]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "734d3d25-6cb3-4474-8547-11ee172e76ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_history = history_list_to_dict(reg_history_list)\n",
    "save_history(reg_history, f'histories/regular_{n_layers}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ee796a-a4d5-4681-856e-cb15fc665977",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Concatenated MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e21e8e-73a5-4fdd-8789-ce82347358a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "with central_storage_strategy.scope():\n",
    "    # Image input layer\n",
    "    inputs = tf.keras.layers.Input(shape=input_shape)\n",
    "    # Context embedding input layer\n",
    "    contexts = tf.keras.layers.Input(shape=(glove_dim))\n",
    "    # ResNet 50 features\n",
    "    features = ImageNet(inputs)\n",
    "    # Concatenate features and context over features dimension\n",
    "    features = tf.concat([features, contexts], axis=-1)\n",
    "    # Number of MLP layers\n",
    "    for layer_idx in range(n_layers):\n",
    "        features = tf.keras.layers.Dense(features.shape[-1], 'elu')(features)\n",
    "    # Output layer\n",
    "    outputs = tf.keras.layers.Dense(n_classes, 'softmax')(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd2b5cb4-b975-438c-8b25-53f97c584c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "con_history_list = []\n",
    "with central_storage_strategy.scope():\n",
    "    # Build model\n",
    "    con_model = tf.keras.models.Model((inputs, contexts), outputs)\n",
    "    # Untrainable layers\n",
    "    for layer in ImageNet.layers:\n",
    "        layer.trainable = False\n",
    "    for decay in range(decays):\n",
    "        # Build optimizer\n",
    "        con_optimizer = tf.keras.optimizers.Adam(learning_rate/2**decay)\n",
    "        # Compile model\n",
    "        con_model.compile(con_optimizer, tf.keras.losses.CategoricalCrossentropy(), metric_fn)\n",
    "        # train\n",
    "        con_history_list.append(\n",
    "            con_model.fit(train_ds, epochs=epochs, verbose=verbose, validation_data=val_ds, callbacks=[]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f04318-4788-4fb4-b771-b60a5388c0a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "con_history = history_list_to_dict(con_history_list)\n",
    "save_history(con_history, f'histories/concatenated_{n_layers}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d687f66-4479-4b73-857d-7af7c08b4085",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Gated MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0242599-1d4d-4251-a58f-b3211a981516",
   "metadata": {},
   "outputs": [],
   "source": [
    "with central_storage_strategy.scope():\n",
    "    # Image input layer\n",
    "    inputs = tf.keras.layers.Input(shape=input_shape)\n",
    "    # Context embedding input layer\n",
    "    contexts = tf.keras.layers.Input(shape=(glove_dim))\n",
    "    # ResNet 50 features\n",
    "    features = ImageNet(inputs)\n",
    "    # Number of MLP layers\n",
    "    for layer_idx in range(n_layers):\n",
    "        features = tf.keras.layers.Dense(features.shape[-1], 'elu')(features)\n",
    "    # Calculate gate values\n",
    "    gates = tf.keras.layers.Dense(features.shape[-1], activation='sigmoid')(contexts)\n",
    "    # Gate features\n",
    "    features = gates * features\n",
    "    # Output layer\n",
    "    outputs = tf.keras.layers.Dense(n_classes, 'softmax')(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a64452c-939f-4b2f-ac27-a8729148e984",
   "metadata": {},
   "outputs": [],
   "source": [
    "gate_history_list = []\n",
    "with central_storage_strategy.scope():\n",
    "    # Build model\n",
    "    gate_model = tf.keras.models.Model((inputs, contexts), outputs)\n",
    "    # Untrainable layers\n",
    "    for layer in ImageNet.layers:\n",
    "        layer.trainable = False\n",
    "    for decay in range(decays):\n",
    "        # Build optimizer\n",
    "        gate_optimizer = tf.keras.optimizers.Adam(learning_rate/2**decay)\n",
    "        # Compile model\n",
    "        gate_model.compile(gate_optimizer, tf.keras.losses.CategoricalCrossentropy(), metric_fn)\n",
    "        # train\n",
    "        gate_history_list.append(\n",
    "            gate_model.fit(train_ds, epochs=epochs, verbose=verbose, validation_data=val_ds, callbacks=[]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9fc8271-7efb-4344-abea-553ab9fc2e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "gate_history = history_list_to_dict(gate_history_list)\n",
    "save_history(gate_history, f'histories/gated_{n_layers}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05eef89e-7328-4e5a-b0ec-1bef63af68d8",
   "metadata": {},
   "source": [
    "### Only tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98533a9d-2bf8-48c7-ba71-727e767e9eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "with central_storage_strategy.scope():\n",
    "    # Image input layer\n",
    "    inputs = tf.keras.layers.Input(shape=input_shape)\n",
    "    # Context embedding input layer\n",
    "    contexts = tf.keras.layers.Input(shape=(glove_dim))\n",
    "    # # ResNet 50 features\n",
    "    # features = ImageNet(inputs)\n",
    "    features = contexts\n",
    "    # Number of CA MLP layers\n",
    "    for layer_idx in range(n_layers):\n",
    "        features = tf.keras.layers.Dense(glove_dim, 'elu')(features)\n",
    "    # Output layer\n",
    "    outputs = tf.keras.layers.Dense(n_classes, 'softmax')(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "417dd510-a6e3-4c42-8052-54fa98033b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_history_list = []\n",
    "with central_storage_strategy.scope():\n",
    "    tag_model = tf.keras.models.Model((inputs, contexts), outputs)\n",
    "    for decay in range(decays):\n",
    "        tag_optimizer = tf.keras.optimizers.Adam(learning_rate*5/2**decay)\n",
    "        tag_model.compile(tag_optimizer, tf.keras.losses.CategoricalCrossentropy(), metric_fn)\n",
    "        tag_history_list.append(\n",
    "            tag_model.fit(train_ds, validation_data = val_ds, epochs=epochs, verbose=verbose, callbacks=[]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3913a212-5781-4957-b6e2-419989cfd2ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_history = history_list_to_dict(tag_history_list)\n",
    "save_history(tag_history, f'histories/only_tags_{n_layers}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b31c9d4-d469-4543-82de-e87e3c91d703",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}